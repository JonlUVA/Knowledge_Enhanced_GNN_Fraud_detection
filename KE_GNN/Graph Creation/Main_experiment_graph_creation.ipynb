{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":648835,"status":"ok","timestamp":1718960568371,"user":{"displayName":"Jonathon Longden","userId":"15152484921944324310"},"user_tz":-120},"id":"ZhoQsSkm5ql3","outputId":"cd68fe24-8f4f-4f92-9869-5f21e9f9fdf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n","Collecting aiohttp (from torch-geometric)\n","  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n","Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n","  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n","  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n","  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n","  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.6.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n","Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch-geometric\n","Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 torch-geometric-2.5.3 yarl-1.9.4\n"]}],"source":["# Mount Google Colab drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Imports\n","import os\n","import numpy as np\n","import pandas as pd\n","import joblib\n","import re\n","\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from time import time\n","import networkx as nx\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from sklearn import tree\n","from sklearn.tree import _tree\n","\n","# Install torch-geometric\n","!pip install torch-geometric\n","from torch_geometric.data import HeteroData\n","\n","from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Change directory to location\n","loc = \"/content/drive/MyDrive/KE_GNN/\"\n","os.chdir(loc)\n","os.getcwd()\n","\n","\n","# how much of a size reduction to the total transaction set\n","size_reduction = 0.3\n","df = pd.read_csv(\"{}clean_processed_transactions.csv\".format(loc)).drop('Unnamed: 0',axis =1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1718960568372,"user":{"displayName":"Jonathon Longden","userId":"15152484921944324310"},"user_tz":-120},"id":"G4C8KF9BDgGD","outputId":"296307d8-6aa1-4dde-dd50-ea3d7842e127"},"outputs":[{"output_type":"stream","name":"stdout","text":["Features NOT being used: \n"]},{"output_type":"execute_result","data":{"text/plain":["['User',\n"," 'Card',\n"," 'Year',\n"," 'Month',\n"," 'Day',\n"," 'Time',\n"," 'Merchant Name',\n"," 'Merchant City',\n"," 'Merchant State',\n"," 'Zip',\n"," 'MCC',\n"," 'Is Fraud?',\n"," 'PK',\n"," 'date_time',\n"," 'DOB',\n"," 'Zipcode',\n"," 'previous_error',\n"," 'user_card',\n"," 'State',\n"," 'Merchant State2',\n"," 'User State2',\n"," 'Fraud2']"]},"metadata":{},"execution_count":2}],"source":["# features for the transactions nodes\n","features = ['transaction_pk',\n","                 'DoW', 'minute', 'number_active_cards', 'number_active_accounts', 'Amount','age at time',  # random varaiables\n","                 'User error_counter',  'User-Merchant error_counter',  'User-Card error_counter',  'User-MCC error_counter', 'Merchant error_counter', # user error counts = a rolling count of pprevious errors\n","                 'User - previous_error', 'User-Merchant - previous_error', 'User-Card - previous_error', 'User-MCC - previous_error', 'Merchant - previous_error',  # indicator if the previous payments were errors\n","                 'User CS', 'User CC', 'User CM', 'User CSTD', 'User CM3', 'User CSTD3', 'User CSTD7', 'User CM7', # user relationship\n","                 'User-Merchant CS', 'User-Merchant CC', 'User-Merchant CM', 'User-Merchant CSTD',  'User-Merchant CM3', 'User-Merchant CSTD3', 'User-Merchant CSTD7', 'User-Merchant CM7', # user- merchant relationship\n","                 'User-Card CS', 'User-Card CC', 'User-Card CM', 'User-Card CSTD', 'User-Card CM3', 'User-Card CSTD3', 'User-Card CSTD7', 'User-Card CM7', # user-card relationship\n","                 'User-MCC CS', 'User-MCC CC', 'User-MCC CM', 'User-MCC CSTD','User-MCC CM3', 'User-MCC CSTD3','User-MCC CSTD7', 'User-MCC CM7', # user - mcc relationship\n","                 'Merchant CS', 'Merchant CC', 'Merchant CM', 'Merchant CSTD', 'Merchant CM3', 'Merchant CSTD3', 'Merchant CSTD7', 'Merchant CM7', # merchant relationship\n","                 'User_lag_tester_payment_1', 'User_lag_tester_payment_5', 'User_lag_tester_payment_10',  'User_lag_tester_payment_20', # user test lag payments\n","                 'User-Merchant_lag_tester_payment_1', 'User-Merchant_lag_tester_payment_5', 'User-Merchant_lag_tester_payment_10', 'User-Merchant_lag_tester_payment_20', # user merchant relationship tester payments\n","                 'User-Card_lag_tester_payment_1', 'User-Card_lag_tester_payment_5', 'User-Card_lag_tester_payment_10', 'User-Card_lag_tester_payment_20', # user cards tester payment\n","                 'User-MCC_lag_tester_payment_1', 'User-MCC_lag_tester_payment_5', 'User-MCC_lag_tester_payment_10', 'User-MCC_lag_tester_payment_20', # user MCC tester payments\n","                 'Merchant_lag_tester_payment_1', 'Merchant_lag_tester_payment_5', 'Merchant_lag_tester_payment_10', 'Merchant_lag_tester_payment_20', # merchant tester paymetns\n","                 'User occurance 1 mins', 'User occurance 10 mins', # user occ in mins\n","                 'User-Merchant occurance 1 mins', 'User-Merchant occurance 10 mins', # user merchant occurance in mins\n","                 'User-Card occurance 1 mins', 'User-Card occurance 10 mins', # user card occurance in mins\n","                 'User-MCC occurance 1 mins', 'User-MCC occurance 10 mins', # user MCC occurance in mins\n","                 'Merchant occurance 1 mins', 'Merchant occurance 10 mins',  # merchants in mins\n","                 'Error - Bad input', 'Error - Insuf bal', 'Error - Tech Glitch' ,\n","                 'OH1: Chip Transaction', 'OH1: Online Transaction', 'OH1: Swipe Transaction',\n","                 'FR: Merchant City', 'FR: Merchant State', 'FR: Zip', 'OH4: Ohio', 'OH4: Online', 'OH4: US',\n","                 'OH4: high_risk', 'OH4: world_non_us', 'OH5: Ohio', 'OH5: US',\n","                 'Per Capita Income - Zipcode','FR: Zipcode', 'Gender','FICO Score', 'Total Debt',\n","                 'FR: MCC', 'OH2: Agricultural Services', 'OH2: Contracted Services', 'OH2: Transportation Services',\n","                 'OH2: Utility Services', 'OH2: Retail Outlet Services', 'OH2: Clothing Stores',\n","                 'OH2: Miscellaneous Stores', 'OH2: Business Services', 'OH2: Professional Services and Membership Organizations', 'OH2: Government Services']\n","\n","# features for users nodes\n","u_features = ['User_index','Per Capita Income - Zipcode','FR: Zipcode', 'Gender','FICO Score', 'Total Debt']\n","\n","# features for the merchant nodes\n","m_features = ['Merchant_index', 'Merchant in Counry',  'FR: MCC',\n","              'OH2: Agricultural Services', 'OH2: Contracted Services', 'OH2: Transportation Services',\n","                 'OH2: Utility Services', 'OH2: Retail Outlet Services', 'OH2: Clothing Stores',\n","                 'OH2: Miscellaneous Stores', 'OH2: Business Services', 'OH2: Professional Services and Membership Organizations',\n","                 'OH2: Government Services' ]\n","# location node features\n","l_features = ['merch_city_index','state_value']\n","\n","# features for the card node\n","c_features = ['user_card_index', 'OH2: Amex', 'OH2: Discover', 'OH2: Mastercard', 'OH2: Visa',\n","              'OH3: Credit', 'OH3: Debit', 'OH3: Debit (Prepaid)','Credit Limit']\n","\n","# total collection of features\n","total_features = features + u_features + m_features + l_features + c_features\n","\n","# prints features not currently being used in the graph structures\n","print('Features NOT being used: ')\n","[x for x in df.columns.tolist() if x not in total_features]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3td-SUmX5rLb","outputId":"3f8e4fd4-3580-4dba-fe9f-6c2070076562","executionInfo":{"status":"ok","timestamp":1718962938732,"user_tz":-120,"elapsed":2370366,"user":{"displayName":"Jonathon Longden","userId":"15152484921944324310"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total size: 17341766 , train len: 12139236, valid len: 2601265, test len: 2601265\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-96b08fd8415c>:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['Merchant State1'] = df['Merchant State'].map(codes)\n","<ipython-input-3-96b08fd8415c>:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['State1'] = df['State'].map(codes)\n"]},{"output_type":"stream","name":"stdout","text":["True\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-96b08fd8415c>:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['Merchant State1'] = df['Merchant State'].map(codes)\n","<ipython-input-3-96b08fd8415c>:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['State1'] = df['State'].map(codes)\n"]},{"output_type":"stream","name":"stdout","text":["True\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-96b08fd8415c>:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['Merchant State1'] = df['Merchant State'].map(codes)\n","<ipython-input-3-96b08fd8415c>:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['State1'] = df['State'].map(codes)\n"]},{"output_type":"stream","name":"stdout","text":["True\n","cpu\n"]}],"source":["# creating an index sorting each user helps with creating graph structure\n","def index_creator(df):\n","\n","  df = df.sort_values(['date_time']).reset_index(drop=True)\n","  df['transaction_pk'] = df.index\n","\n","  index_map = {}\n","  assigned_index = 0\n","\n","  # Assign the incremental index starting from 0 for each primary key\n","  for key in df['User'].unique():\n","      index_map[key] = assigned_index\n","      assigned_index += 1\n","\n","  # Map the primary keys to their corresponding starting index\n","  df['User_index'] = df['User'].map(index_map)\n","\n","  index_map = {}\n","  assigned_index = 0\n","  #assigned_index = max(df['User_index']) + 1\n","\n","  # Assign the incremental index starting from 0 for each primary key\n","  for key in df['Merchant Name'].unique():\n","      index_map[key] = assigned_index\n","      assigned_index += 1\n","\n","  # Map the primary keys to their corresponding starting index\n","  df['Merchant_index'] = df['Merchant Name'].map(index_map)\n","\n","\n","\n","  index_map = {}\n","  assigned_index = 0\n","\n","  index_map = {}\n","  assigned_index = 0\n","  #assigned_index = max(df['User_index']) + 1\n","\n","  # Assign the incremental index starting from 0 for each primary key\n","  for key in df['user_card'].unique():\n","      index_map[key] = assigned_index\n","      assigned_index += 1\n","\n","  # Map the primary keys to their corresponding starting index\n","  df['user_card_index'] = df['user_card'].map(index_map)\n","  return df\n","\n","def graph_maker(df,test_train):\n","  '''\n","  input is the df that is to be made into a graph\n","  test_train is either train or test\n","  '''\n","  data = HeteroData() # Full graph\n","\n","\n","\n","  #user <-> card\n","  user_edge_index = df[['User_index', 'user_card_index']].drop_duplicates().to_numpy().T\n","  rev_user_edge_index = df[['user_card_index', 'User_index']].drop_duplicates().to_numpy().T\n","\n","  data['user', 'owns', 'card'].edge_index = torch.from_numpy(user_edge_index)\n","  data['card', 'rev_own', 'user'].edge_index = torch.from_numpy(rev_user_edge_index)\n","\n","  #card <-> transaction\n","  card_edge_index = df[['user_card_index', 'transaction_pk']].drop_duplicates().to_numpy().T\n","  rev_card_edge_index = df[['transaction_pk', 'user_card_index']].drop_duplicates().to_numpy().T\n","\n","  data['card', 'transfer', 'transaction'].edge_index = torch.from_numpy(card_edge_index)\n","  data['transaction', 'rev_transfer', 'card'].edge_index = torch.from_numpy(rev_card_edge_index)\n","\n","\n","\n","\n","  # creating the location information I think\n","  df = df.astype({\n","              'Merchant State': 'category',\n","              'State': 'category',\n","            })\n","\n","  codes = dict(zip(df['Merchant State'], df['Merchant State'].cat.codes))\n","  df['Merchant State1'] = df['Merchant State'].map(codes)\n","  df['State1'] = df['State'].map(codes)\n","  df = df.astype({\n","              'Merchant State1': 'int64',\n","              'State1': 'int64',\n","            })\n","\n","\n","  #person <-> location\n","  location_edge_index1 = df[['User_index', 'State1']].drop_duplicates().to_numpy().T\n","  rev_location_edge_index1 = df[['State1', 'User_index']].drop_duplicates().to_numpy().T\n","\n","\n","\n","  data['user', 'happened_at', 'location'].edge_index = torch.from_numpy(location_edge_index1)\n","  data['location', 'rev_happend_at', 'user'].edge_index = torch.from_numpy(rev_location_edge_index1)\n","\n","\n","  #merchant <-> transaction\n","  merchant_edge_index = df[['Merchant_index', 'transaction_pk']].drop_duplicates().to_numpy().T\n","  rev_merchant_edge_index = df[['transaction_pk', 'Merchant_index']].drop_duplicates().to_numpy().T\n","\n","  data['merchant', 'transfer', 'transaction'].edge_index = torch.from_numpy(merchant_edge_index)\n","  data['transaction', 'rev_transfer', 'merchant'].edge_index = torch.from_numpy(rev_merchant_edge_index)\n","\n","\n","  # user <-> transaction\n","  user_transaction_edge_index = df[['User_index', 'transaction_pk']].drop_duplicates().to_numpy().T\n","  rev_user_transaction_edge_index = df[['transaction_pk', 'User_index']].drop_duplicates().to_numpy().T\n","\n","  data['user', 'bought', 'transaction'].edge_index = torch.from_numpy(user_transaction_edge_index)\n","  data['transaction', 'rev_bought', 'user'].edge_index = torch.from_numpy(rev_user_transaction_edge_index)\n","  # user <-> merchant\n","  user_merchant_edge_index = df[['User_index', 'Merchant_index']].drop_duplicates().to_numpy().T\n","  rev_user_merchant_edge_index = df[['Merchant_index', 'User_index']].drop_duplicates().to_numpy().T\n","\n","  data['user', 'bought_from', 'merchant'].edge_index = torch.from_numpy(user_merchant_edge_index)\n","  data['merchant', 'rev_bought_from', 'user'].edge_index = torch.from_numpy(rev_user_merchant_edge_index)\n","\n","  # transaction <-> loc\n","  transaction_loc_edge_index = df[['transaction_pk', 'Merchant State1']].drop_duplicates().to_numpy().T\n","  rev_transaction_loc_edge_index = df[['Merchant State1', 'transaction_pk']].drop_duplicates().to_numpy().T\n","\n","  data['transaction', 'bought_in', 'location'].edge_index = torch.from_numpy(transaction_loc_edge_index)\n","  data['location', 'rev_bought_in', 'transaction'].edge_index = torch.from_numpy(rev_transaction_loc_edge_index)\n","\n","  # card <-> merchant\n","\n","  card_merchant_edge_index = df[['user_card_index', 'Merchant_index']].drop_duplicates().to_numpy().T\n","  rev_card_merchant_edge_index = df[['Merchant_index', 'user_card_index']].drop_duplicates().to_numpy().T\n","\n","  data['card', 'bought_with', 'merchant'].edge_index = torch.from_numpy(card_merchant_edge_index)\n","  data['merchant', 'rev_bought_with', 'card'].edge_index = torch.from_numpy(rev_card_merchant_edge_index)\n","\n","  # location <-> location\n","\n","  locs_edge_index = df[['State1', 'Merchant State1']].drop_duplicates().to_numpy().T\n","  rev_locs_edge_index = df[['Merchant State1', 'State1']].drop_duplicates().to_numpy().T\n","\n","  data['location', 'at', 'location'].edge_index = torch.from_numpy(locs_edge_index)\n","  data['location', 'rev_at', 'location'].edge_index = torch.from_numpy(rev_locs_edge_index)\n","\n","  # transaction features\n","  x = df[features].drop('transaction_pk',axis=1).to_numpy(dtype='float32')\n","  y = df['Is Fraud?'].to_numpy(dtype='float32').reshape(-1,1)\n","  data['transaction'].x = torch.from_numpy(x)\n","  data['transaction'].y = torch.from_numpy(y)\n","\n","  #merchant features\n","  merchant_data = df[m_features].sort_values('Merchant_index').drop_duplicates(subset=['Merchant_index']).to_numpy(dtype='float32')[:,1:]\n","  data['merchant'].x = torch.from_numpy(merchant_data)\n","  #features\n","  user_data = df[u_features].sort_values('User_index').drop_duplicates(subset=['User_index']).to_numpy(dtype='float32')[:,1:]\n","  data['user'].x = torch.from_numpy(user_data)\n","\n","  #card\n","  card_data = df[c_features].sort_values('user_card_index').drop_duplicates(subset=['user_card_index']).to_numpy(dtype='float32')[:,1:]\n","  data['card'].x = torch.from_numpy(card_data)\n","  #location\n","\n","  data['location'].x = torch.from_numpy(np.ones((len(df['Merchant State'].unique()), 1), dtype='float32'))\n","  # masks\n","  if test_train == 'train':\n","    a = int(len(df))\n","    b = 0\n","    train_mask = np.concatenate((np.ones(a, dtype=bool), np.zeros(b, dtype=bool)), axis=0)\n","    data['transaction'].train_mask = torch.from_numpy(train_mask)\n","  elif test_train == 'test':\n","    a = 0\n","    b = int(len(df))\n","    test_mask = np.concatenate((np.ones(a, dtype=bool), np.zeros(b, dtype=bool)), axis=0)\n","    data['transaction'].test_mask = torch.from_numpy(test_mask)\n","  print(data.validate())\n","  return data, df[features].drop('transaction_pk',axis=1).columns.tolist()\n","\n","df = df.sort_values(['date_time']).reset_index(drop=True)\n","a = int(len(df) * (1- size_reduction))\n","df_reduced = df[:a]\n","\n","df_train_t = df_reduced[0: int(len(df_reduced)*.70)]\n","df_train_v = df_reduced[int(len(df_reduced)*.70): int(len(df_reduced)*.85)]\n","df_test = df_reduced[int(len(df_reduced)*.85):]\n","print('total size: {} , train len: {}, valid len: {}, test len: {}'.format(len(df_reduced), len(df_train_t),\n","                                                                           len(df_train_v), len(df_test)))\n","\n","df_train_t = index_creator(df_train_t)\n","df_train_v = index_creator(df_train_v)\n","\n","train_data, transaction_features = graph_maker(df_train_t,'train')\n","valid_data, transaction_features = graph_maker(df_train_v,'train')\n","\n","df_test = index_creator(df_test)\n","test_data, transaction_features = graph_maker(df_test,'test')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)\n","\n","torch.save(test_data, '{}Graph storage/test_graph.pt'.format(loc))\n","torch.save(train_data, '{}Graph storage/train_graph.pt'.format(loc))\n","torch.save(valid_data, '{}Graph storage/valid_graph.pt'.format(loc))\n","\n","#save df test, train, valid for XGBOOST model\n","df_train_t.to_csv('{}Graph storage/train_df.csv'.format(loc),index=False)\n","df_train_v.to_csv('{}Graph storage/valid_df.csv'.format(loc),index=False)\n","df_test.to_csv('{}Graph storage/test_df.csv'.format(loc), index=False)\n","\n"]},{"cell_type":"markdown","source":["Creating the random forest to be used for the clauses.\n","\n","\n"],"metadata":{"id":"_AsIMYzj6ZA5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266240,"status":"ok","timestamp":1718885440048,"user":{"displayName":"Jonathon Longden","userId":"15152484921944324310"},"user_tz":-120},"id":"VR1sKME_IOkl","outputId":"ee0ce949-e66c-4259-cceb-33adfb69d452"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-4c4ee8ea77c8>:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  dec_model.fit(df_train_t[transaction_features],\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00   2598056\n","           1       0.99      0.42      0.59      3209\n","\n","    accuracy                           1.00   2601265\n","   macro avg       1.00      0.71      0.80   2601265\n","weighted avg       1.00      1.00      1.00   2601265\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/KE_GNN/clause_random_forest.joblib']"]},"metadata":{},"execution_count":4}],"source":["\n","dec_model = RandomForestClassifier(n_estimators=100, max_depth=10,\n","                                    min_samples_split = 10,\n","                                   random_state=420, n_jobs=-1)\n","dec_model.fit(df_train_t[transaction_features],\n","              df_train_t[['Is Fraud?']])\n","\n","# ensuring that model was trained correctly\n","y_pred = dec_model.predict(df_train_v[transaction_features])\n","\n","print(classification_report(df_train_v[['Is Fraud?']], y_pred))\n","# save\n","joblib.dump(dec_model, '{}clause_random_forest.joblib'.format(loc))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1913,"status":"ok","timestamp":1718886783930,"user":{"displayName":"Jonathon Longden","userId":"15152484921944324310"},"user_tz":-120},"id":"FsEXSHHWNGDb","outputId":"e2fbfc97-ea82-4c45-e779-58456edb3526"},"outputs":[{"output_type":"stream","name":"stdout","text":["   index                                               rule   fraud   Perc  \\\n","0   1714  if (OH4: high_risk <= 0.5) and (FR: Merchant S...  1Fraud  100.0   \n","1   1885  if (FR: Merchant State <= 0.546) and (FR: Merc...  1Fraud  100.0   \n","2   1984  if (User-Merchant CSTD > 0.0) and (OH1: Swipe ...  1Fraud  100.0   \n","3   2880  if (OH4: high_risk <= 0.5) and (Merchant CSTD ...  1Fraud  100.0   \n","4   5483  if (OH1: Swipe Transaction <= 0.5) and (FR: MC...  1Fraud  100.0   \n","5   5717  if (OH1: Online Transaction > 0.5) and (OH2: C...  1Fraud  100.0   \n","6   6291  if (Merchant CSTD7 > 0.018) and (FR: Merchant ...  1Fraud  100.0   \n","7   6966  if (User-Merchant CM > 0.013) and (OH1: Online...  1Fraud  100.0   \n","8   7762  if (OH4: US <= 0.5) and (Merchant CSTD <= 0.08...  1Fraud  100.0   \n","9   8459  if (FR: Zip <= 0.379) and (FR: MCC <= 0.05) an...  1Fraud  100.0   \n","\n","   sample  \n","0    1040  \n","1     756  \n","2     837  \n","3     750  \n","4     806  \n","5    1051  \n","6     966  \n","7     996  \n","8     819  \n","9     832  \n"]},{"output_type":"execute_result","data":{"text/plain":["{'RULE1': [[('OH4: high_risk', 98), '<=', 0.5],\n","  [('FR: Merchant State', 93), '>', 0.004],\n","  [('OH2: Contracted Services', 109), '>', 0.5],\n","  [('FR: Merchant City', 92), '>', 0.003],\n","  [('FR: Zipcode', 103), '>', 0.001],\n","  [('OH4: world_non_us', 99), '<=', 0.5]],\n"," 'RULE2': [[('FR: Merchant State', 93), '<=', 0.546],\n","  [('FR: Merchant City', 92), '>', 0.003],\n","  [('User-Merchant CSTD3', 29), '>', 0.0],\n","  [('FR: MCC', 107), '>', 0.001],\n","  [('Merchant CC', 49), '>', 0.092],\n","  [('OH4: Online', 96), '>', 0.5]],\n"," 'RULE3': [[('User-Merchant CSTD', 27), '>', 0.0],\n","  [('OH1: Swipe Transaction', 91), '<=', 0.5],\n","  [('OH2: Contracted Services', 109), '>', 0.5]],\n"," 'RULE4': [[('OH4: high_risk', 98), '<=', 0.5],\n","  [('Merchant CSTD', 51), '<=', 0.193],\n","  [('FR: Merchant City', 92), '>', 0.003],\n","  [('OH2: Contracted Services', 109), '>', 0.5],\n","  [('FR: Zip', 94), '>', 0.009],\n","  [('FR: Merchant State', 93), '>', 0.006],\n","  [('User CM3', 20), '<=', 0.041],\n","  [('FR: Zipcode', 103), '>', 0.0],\n","  [('OH4: world_non_us', 99), '<=', 0.5]],\n"," 'RULE5': [[('OH1: Swipe Transaction', 91), '<=', 0.5],\n","  [('FR: MCC', 107), '<=', 0.008],\n","  [('Merchant CC', 49), '<=', 0.263],\n","  [('OH2: Contracted Services', 109), '<=', 0.5],\n","  [('Merchant CM', 50), '>', 0.007],\n","  [('FR: MCC', 107), '>', 0.001],\n","  [('FR: MCC', 107), '<=', 0.004]],\n"," 'RULE6': [[('OH1: Online Transaction', 90), '>', 0.5],\n","  [('OH2: Contracted Services', 109), '>', 0.5]],\n"," 'RULE7': [[('Merchant CSTD7', 54), '>', 0.018],\n","  [('FR: Merchant City', 92), '<=', 0.525],\n","  [('OH1: Online Transaction', 90), '>', 0.5],\n","  [('OH2: Contracted Services', 109), '>', 0.5]],\n"," 'RULE8': [[('User-Merchant CM', 26), '>', 0.013],\n","  [('OH1: Online Transaction', 90), '>', 0.5],\n","  [('OH2: Contracted Services', 109), '>', 0.5]],\n"," 'RULE9': [[('OH4: US', 97), '<=', 0.5],\n","  [('Merchant CSTD', 51), '<=', 0.086],\n","  [('FR: MCC', 107), '<=', 0.019],\n","  [('User-Merchant CSTD7', 30), '>', 0.0],\n","  [('FR: Merchant City', 92), '<=', 0.448],\n","  [('OH2: Contracted Services', 109), '>', 0.5],\n","  [('FR: Merchant State', 93), '>', 0.006],\n","  [('Merchant CC', 49), '>', 0.001],\n","  [('OH1: Swipe Transaction', 91), '<=', 0.5]],\n"," 'RULE10': [[('FR: Zip', 94), '<=', 0.379],\n","  [('FR: MCC', 107), '<=', 0.05],\n","  [('FR: MCC', 107), '>', 0.003],\n","  [('User-Merchant CSTD3', 29), '>', 0.0],\n","  [('User-Merchant CM7', 31), '>', 0.009],\n","  [('OH1: Swipe Transaction', 91), '<=', 0.5],\n","  [('User-MCC CM', 42), '>', 0.005],\n","  [('OH2: Contracted Services', 109), '>', 0.5]]}"]},"metadata":{},"execution_count":6}],"source":["# Function to extract decision rules from a tree model\n","def get_rules(tree, feature_names, class_names):\n","    rules = []  # List to store the extracted rules\n","    ruleD = {}  # Dictionary to store the details of the rules\n","\n","    # Loop through each estimator in the tree ensemble\n","    for tree_idx, est in enumerate(tree.estimators_):\n","        tree_ = est.tree_  # Get the tree structure\n","        feature_name = [\n","            feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n","            for i in tree_.feature\n","        ]  # Get the feature names, handling undefined features\n","\n","        paths = []  # List to store all paths (rules) in the tree\n","        path = []  # List to store the current path\n","\n","        # Recursive function to traverse the tree and extract paths\n","        def recurse(node, path, paths):\n","            if tree_.feature[node] != _tree.TREE_UNDEFINED:  # If the node is not a leaf\n","                name = feature_name[node]\n","                threshold = tree_.threshold[node]\n","                p1, p2 = list(path), list(path)  # Create copies of the current path\n","                p1 += [f\"({name} <= {np.round(threshold, 3)})\"]  # Append condition for left child\n","                recurse(tree_.children_left[node], p1, paths)  # Recurse to the left child\n","                p2 += [f\"({name} > {np.round(threshold, 3)})\"]  # Append condition for right child\n","                recurse(tree_.children_right[node], p2, paths)  # Recurse to the right child\n","            else:\n","                path += [(tree_.value[node], tree_.n_node_samples[node])]  # Append leaf node value and sample count\n","                paths += [path]  # Add the complete path to paths\n","\n","        recurse(0, path, paths)  # Start recursion from the root node\n","\n","        # Sort paths by the number of samples in descending order\n","        samples_count = [p[-1][1] for p in paths]\n","        ii = list(np.argsort(samples_count))\n","        paths = [paths[i] for i in reversed(ii)]\n","\n","        # Construct rules from paths\n","        for path in paths:\n","            rule = \"if \"\n","            for p in path[:-1]:\n","                if rule != \"if \":\n","                    rule += \" and \"\n","                rule += str(p)\n","            rule += \" then \"\n","            if class_names is None:\n","                rule += \"response: \" + str(np.round(path[-1][0][0][0], 3))  # Add response value if class names are not provided\n","            else:\n","                classes = path[-1][0][0]\n","                l = np.argmax(classes)\n","                rule += f\"class: {class_names[l]} (proba: {np.round(100.0 * classes[l] / np.sum(classes), 2)}%)\"  # Add class and probability\n","            rule += f\" | based on {path[-1][1]:,} samples\"  # Add sample count\n","            if class_names[l] == '1Fraud':\n","                rules += [rule]  # Add the rule to the list if the class is '1Fraud'\n","                ruleD[rule] = [class_names[l], np.round(100.0 * classes[l] / np.sum(classes), 2), path[-1][1]]  # Add rule details to the dictionary\n","\n","    return rules, ruleD  # Return the extracted rules and their details\n","\n","rules, ruleDic = get_rules(dec_model, transaction_features, ['Non-Fraud', '1Fraud'])\n","\n","\n","def extract_conditions(string):\n","  '''\n","  cleans the string and extracts the conditions\n","  '''\n","  conditions = []\n","  pattern = r'\\((.*?)\\)'\n","  matches = re.findall(pattern, string)[:-1]\n","  for match in matches:\n","      parts = match.split()\n","      variable_name = match.replace(parts[-2], '').replace(parts[-1], '').strip()\n","      conditions.append([variable_name, parts[-2], float(parts[-1])])\n","  return conditions\n","\n","# Extract conditions from the string column\n","rule_df = pd.DataFrame.from_dict(ruleDic, orient='index').reset_index()\n","rule_df.columns = ['rule','fraud','Perc','sample']\n","\n","def rule_extraction(df,num, sample=500, perc = 90):\n","  '''\n","  extracts the rules that meet the criteria\n","  '''\n","  df = df[df['sample'] >= sample]\n","  df = df[df['Perc'] >= perc]\n","  df = df.sort_values('Perc', ascending = False)[:num].reset_index()\n","  print(df)\n","  rule_dic = {}\n","  for index,row in df.iterrows():\n","    rule_number = 1 + index\n","    rule_dic['RULE{}'.format(rule_number)] = extract_conditions(row['rule'])\n","  return rule_dic\n","\n","first_dict = rule_extraction(df = rule_df, num = 10,  sample = 750, perc = 90)\n","first_dict\n","def add_feature_location(features, rule_dict):\n","  '''adds the location (index) of each feature on the transaction node'''\n","  final_dict = {}\n","  for k, v in rule_dict.items():\n","    conditions = list()\n","    for x in v:\n","      conditions2 = list()\n","      conditions2.append((x[0], features.index(x[0])))\n","      conditions2.append(x[1])\n","      conditions2.append(x[2])\n","      conditions.append(conditions2)\n","      final_dict[k] = conditions\n","  return final_dict\n","final_dict = add_feature_location(transaction_features, first_dict)\n","final_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2256,"status":"ok","timestamp":1718886895055,"user":{"displayName":"Jonathon Longden","userId":"15152484921944324310"},"user_tz":-120},"id":"8itkHJppNJ6x","outputId":"1acff650-71f4-4d2c-f940-f89ac560ce34"},"outputs":[{"output_type":"stream","name":"stdout","text":["dictionary saved successfully to file\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-cec22014baa0>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return torch.tensor(all_conditions_met, dtype=torch.float32).view(-1, 1)\n"]},{"output_type":"stream","name":"stdout","text":["dictionary saved successfully to file\n","dictionary saved successfully to file\n","dictionary saved successfully to file\n"]}],"source":["import pickle\n","\n","with open('{}Clause Storage/Knowledge_enhancements_large.pkl'.format(loc), 'wb') as fp:\n","    pickle.dump(final_dict, fp)\n","    print('dictionary saved successfully to file')\n","\n","\n","\n","def filter_transactions(x_dict, conditions):\n","  '''\n","  This finds the location of the transactions in the graph that meet the conditions\n","  '''\n","  filtered_indices_list = []\n","  for condition in conditions:\n","      (column_name, column_index), operator, value = condition\n","      column_values = x_dict[:, column_index]\n","\n","      if operator == '>':\n","          condition_met = column_values > float(value)\n","      elif operator == '<':\n","          condition_met = column_values < float(value)\n","      elif operator == '>=':\n","          condition_met = column_values >= float(value)\n","      elif operator == '<=':\n","          condition_met = column_values <= float(value)\n","      elif operator == '==':\n","          condition_met = column_values == float(value)\n","      else:\n","          raise ValueError(f\"Invalid operator: {operator}\")\n","\n","      filtered_indices_list.append(condition_met)\n","\n","  filtered_indices = torch.stack(filtered_indices_list, dim=1)\n","\n","  # Check if all conditions are met for each row\n","  all_conditions_met = torch.all(filtered_indices, dim=1)\n","  return torch.tensor(all_conditions_met, dtype=torch.float32).view(-1, 1)\n","\n","condition_train = {}\n","condition_valid = {}\n","condition_test = {}\n","for k, v in final_dict.items():\n","  condition_train[k] = filter_transactions(train_data.x_dict['transaction'], v)\n","  condition_valid[k] = filter_transactions(valid_data.x_dict['transaction'], v)\n","  condition_test[k] = filter_transactions(test_data.x_dict['transaction'], v)\n","\n","\n","\n","with open('{}Clause Storage/train_KE_location_large.pkl'.format(loc), 'wb') as fp:\n","    pickle.dump(condition_train, fp)\n","    print('dictionary saved successfully to file')\n","\n","\n","with open('{}Clause Storage/valid_KE_location_large.pkl'.format(loc), 'wb') as fp:\n","    pickle.dump(condition_valid, fp)\n","    print('dictionary saved successfully to file')\n","\n","\n","\n","with open('{}Clause Storage/test_KE_location_large.pkl'.format(loc), 'wb') as fp:\n","    pickle.dump(condition_test, fp)\n","    print('dictionary saved successfully to file')\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"nOKtwyjeoufg"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOmlop2IExZvgaIoQEJVmuK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}